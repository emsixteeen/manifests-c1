IMPORTANT: Ensure that on ALL nodes the directory /var/lib/rook does not exist and/or is empty. If it is not, Ceph will NOT work correctly. Prior to running any of the installation steps below, it is imperative that the directory /var/lib/rook does not exist or is empty on ALL nodes, or installation will fail.

Install as ordinary Kubeflow, but instead of using "kustomize build | kubectl apply -f examples/", replace "example/" with "custom/".

E.g.: 

$ git checkout <repo>
$ cd <repo>
$ git checkout c1
$ while ! kustomize build custom/ | kubectl apply -f -; do echo "Waiting to complete..."; sleep 5; done

NOTE: to change the Ceph cluster from a single-node to a multi-node cluster, edit the ceph/kustomization.yaml file, and change the bases from single-node/ to multi-node/.

After a while, all pods in the rook-ceph namespace will be running, but in the kubeflow namespace there will be some with a CrashLookBackoff, or similar.

Once ther pods in the rook-ceph namespace are all running, an update needs to be made to the minio secret in the kubeflow namespace, to get the pods in the kubeflow namespace running.

Execute:

$ kubectl -n kubeflow create secret generic mlpipeline-minio-artifact \
  --save-config \
  --dry-run=client \
  --output yaml \
  --from-literal=accesskey=$(kubectl -n rook-ceph get secret rook-ceph-object-user-minio-minio -o jsonpath='{.data.AccessKey}' | base64 -d) \
  --from-literal=secretkey=$(kubectl -n rook-ceph get secret rook-ceph-object-user-minio-minio -o jsonpath='{.data.SecretKey}' | base64 -d) \
  | kubectl apply -f -
